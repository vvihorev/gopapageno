package generator

import (
	"bufio"
	"fmt"
	"github.com/giornetta/gopapageno/generator/regex"
	"io"
	"log"
	"regexp"
	"sort"
	"strings"
)

var (
	separatorRegexp = regexp.MustCompile("^%%\\s*$")
	cutPointsRegex  = regexp.MustCompile("^%cut\\s*(\\S.*)$")
	definitionRegex = regexp.MustCompile("^([a-zA-Z][a-zA-Z0-9]*)\\s*(.+)$")
)

type lexerDescriptor struct {
	rules     []lexRule
	cutPoints string
	code      string

	// dfa is nil until compile() is executed successfully.
	dfa regex.Dfa

	// cutPointsDfa is nil until compile() is executed successfully.
	cutPointsDfa regex.Dfa
}

// A lexRule matches a specific regex pattern to a semantic action to be performed during lexing.
type lexRule struct {
	Regex  string
	Action string
}

func (r lexRule) String() string {
	return fmt.Sprintf("%s: %s", r.Regex, r.Action)
}

func parseLexerDescription(r io.Reader, logger *log.Logger) (*lexerDescriptor, error) {
	logger.Printf("Parsing lexer description file...\n")

	scanner := bufio.NewScanner(r)

	//Scan the definitions section
	cutPoints := ""
	definitions := make(map[string]string)

	for scanner.Scan() {
		l := scanner.Text()
		if separatorRegexp.MatchString(l) {
			break
		}

		defMatch := definitionRegex.FindStringSubmatch(l)
		cutPointsMatch := cutPointsRegex.FindStringSubmatch(l)
		if defMatch != nil {
			definitions[defMatch[1]] = strings.TrimSpace(defMatch[2])
		} else if cutPointsMatch != nil {
			cutPoints = cutPointsMatch[1]
		}
	}

	logger.Printf("Cut Points: %s\n", cutPoints)

	logger.Printf("Definitions:\n")
	for k, v := range definitions {
		logger.Printf("%s: %s\n", k, v)
	}

	var sb strings.Builder

	// Scan the rules section
	for scanner.Scan() {
		l := scanner.Bytes()
		if separatorRegexp.Match(l) {
			break
		}

		sb.Write(l)
		sb.WriteString("\n")
	}

	lexRules, err := parseLexRules(sb.String(), definitions)
	if err != nil {
		return nil, fmt.Errorf("could not parse lexer rules: %w", err)
	}

	logger.Printf("Lex Rules:\n")
	for _, rule := range lexRules {
		logger.Printf("%s\n", rule)
	}

	// Scan the code section
	sb.Reset()

	for scanner.Scan() {
		l := scanner.Bytes()

		sb.Write(l)
		sb.WriteString("\n")
	}

	code := sb.String()

	return &lexerDescriptor{
		rules:     lexRules,
		cutPoints: cutPoints,
		code:      code,
	}, nil
}

func parseLexRules(input string, definitions map[string]string) ([]lexRule, error) {
	lexRules := make([]lexRule, 0)

	var pos int
	skipSpaces(input, &pos)

	var regexBuilder strings.Builder

	for pos < len(input) {
		startingPos := pos

		// Read anything until a { is reached
		for pos < len(input) && input[pos] != '{' {
			pos++
		}

		if pos >= len(input) {
			break
		}

		// When a { is reached, try to read a definition and then a }
		// If it's not possible then the regex part is over
		leftCurlyPos := pos

		pos++
		identifier := getIdentifier(input, &pos)

		rightCurlyPos := pos

		if input[rightCurlyPos] == '}' && identifier != "" {
			definition, ok := definitions[identifier]
			if !ok {
				return nil, fmt.Errorf("missing definition for %s", identifier)
			}

			regexBuilder.WriteString(input[startingPos:leftCurlyPos])
			regexBuilder.WriteString(definition)

			pos++
		} else {
			pos = leftCurlyPos
			semFun := getSemanticFunction(input, &pos)

			regexBuilder.WriteString(input[startingPos:leftCurlyPos])

			lexRules = append(lexRules, lexRule{
				Regex:  strings.Trim(regexBuilder.String(), " \t\r\n"),
				Action: semFun,
			})

			regexBuilder.Reset()

			skipSpaces(input, &pos)
		}
	}

	return lexRules, nil
}

// compile builds a DFA from the rules defined in the description file.
// It uses a Regexp parser generated by Gopapageno itself (don't ask me why, I tried refactoring this but it's hard, I'm hiding this here for now)
// TODO: Build a hand-written Regexp Parser?
func (l *lexerDescriptor) compile() error {
	if len(l.rules) <= 0 {
		return fmt.Errorf("the lexer descriptor does not contain any rules")
	}

	success, result := regex.ParseString([]byte(l.rules[0].Regex), 1)
	if !success {
		return fmt.Errorf("could not parse regular expression %s", l.rules[0].Regex)
	}

	nfa := result.Value.(*regex.Nfa)
	nfa.AddAssociatedRule(0)

	for i := 1; i < len(l.rules); i++ {
		success, result := regex.ParseString([]byte(l.rules[i].Regex), 1)
		if !success {
			return fmt.Errorf("could not parse regular expression %s", l.rules[i].Regex)
		}

		curNfa := result.Value.(*regex.Nfa)
		curNfa.AddAssociatedRule(i)

		nfa.Unite(*curNfa)
	}

	dfa := nfa.ToDfa()

	var cutPointsDfa regex.Dfa
	if l.cutPoints == "" {
		cutPointsNfa := regex.NewEmptyStringNfa()
		cutPointsDfa = cutPointsNfa.ToDfa()
	} else {
		success, result := regex.ParseString([]byte(l.cutPoints), 1)
		if !success {
			return fmt.Errorf("could not parse regular expression %s", l.rules[0].Regex)
		}

		cutPointsNfa := result.Value.(*regex.Nfa)
		cutPointsDfa = cutPointsNfa.ToDfa()
	}

	l.dfa = dfa
	l.cutPointsDfa = cutPointsDfa

	return nil
}

func (l *lexerDescriptor) emit(f io.Writer, opts *Options) {
	fmt.Fprintf(f, l.code)

	fmt.Fprintf(f, "\n\nfunc NewLexer() *gopapageno.Lexer {\n")

	/************
	 * Automata *
	 ************/
	fmt.Fprintf(f, "\tautomaton := ")
	emitAutomata(f, l.dfa)

	fmt.Fprintf(f, "\tcutPointsAutomaton := ")
	emitAutomata(f, l.cutPointsDfa)

	/******************
	 * Lexer Function *
	 ******************/
	fmt.Fprintf(f, "\tfn := func(rule int, text string, start int, end int, thread int, token *gopapageno.Token) gopapageno.LexResult {\n")
	fmt.Fprintf(f, "\t\ttoken.Type = gopapageno.TokenTerm\n")
	//fmt.Fprintf(f, "\t\ttoken.Lexeme = text\n\n")
	fmt.Fprintf(f, "\t\tswitch rule {\n")
	for i, rule := range l.rules {
		fmt.Fprintf(f, "\t\tcase %d:\n", i)
		for _, line := range strings.Split(rule.Action, "\n") {
			fmt.Fprintf(f, "\t\t\t%s\n", line)
		}
	}
	fmt.Fprintf(f, "\t\tdefault:\n\t\t\treturn gopapageno.LexErr\n\t\t}\n\n")
	fmt.Fprintf(f, "\t\treturn gopapageno.LexOK\n\t}\n\n")

	/****************
	 * Return Lexer *
	 ****************/
	fmt.Fprintf(f, "\treturn &gopapageno.Lexer{\n")
	fmt.Fprintf(f, "\t\tAutomaton: automaton,\n")
	fmt.Fprintf(f, "\t\tCutPointsAutomaton: cutPointsAutomaton,\n")
	fmt.Fprintf(f, "\t\tFunc: fn,\n")

	if !opts.TypesOnly {
		fmt.Fprintf(f, "\t\tPreallocFunc: LexerPreallocMem,\n")
	}

	fmt.Fprintf(f, "\t}\n}\n")
}

func emitAutomata(f io.Writer, dfa regex.Dfa) {
	fmt.Fprintf(f, "[]gopapageno.LexerDFAState{\n")

	states := dfa.GetStates()
	for _, state := range states {
		fmt.Fprintf(f, "\t\t{[256]int{")
		for i := 0; i < len(state.Transitions)-1; i++ {
			if state.Transitions[i] == nil {
				fmt.Fprintf(f, "-1, ")
			} else {
				fmt.Fprintf(f, "%d, ", state.Transitions[i].Num)
			}
		}
		if state.Transitions[len(state.Transitions)-1] == nil {
			fmt.Fprintf(f, "-1")
		} else {
			fmt.Fprintf(f, "%d", state.Transitions[len(state.Transitions)-1].Num)
		}

		fmt.Fprintf(f, "}, %t, []int{", state.IsFinal)
		sort.Ints(state.AssociatedRules)
		for i := 0; i < len(state.AssociatedRules)-1; i++ {
			fmt.Fprintf(f, "%d, ", state.AssociatedRules[i])
		}
		if len(state.AssociatedRules) > 0 {
			fmt.Fprintf(f, "%d", state.AssociatedRules[len(state.AssociatedRules)-1])
		}
		fmt.Fprintf(f, "}},\n")
	}
	fmt.Fprintf(f, "\t}\n\n")
}
